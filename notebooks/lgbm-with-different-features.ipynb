{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/cast42/xgboost-in-python-with-rmspe-v2\n",
    "\n",
    "https://www.kaggle.com/cast42/xgboost-extra-features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import random\n",
    "import lightgbm as lgb\n",
    "gc.enable()\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {}\n",
    "param['application'] = 'regression_l2'\n",
    "param['learning_rate'] = 0.1\n",
    "param['feature_fraction'] = 0.5\n",
    "param['bagging_fraction'] = 0.5\n",
    "param['bagging_freq'] = 1\n",
    "param['max_depth'] = 5\n",
    "param['num_threads'] = 4\n",
    "param['verbose'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"data/train_1.csv\")\n",
    "df = pd.melt(df, id_vars='Page', var_name='date', value_name='Visits')\n",
    "df['date'] = df['date'].astype('datetime64[ns]')\n",
    "df['Visits'] = df['Visits'].astype('float32')\n",
    "\n",
    "temp = df.loc[(df[\"Visits\"] > 0) & (df[\"date\"] < '2016-03-01')].groupby([\"Page\"]).size()\n",
    "pages = list(temp.index)\n",
    "df = df.loc[(df[\"date\"] > '2015-03-01') & (df[\"Page\"].isin(pages))]\n",
    "\n",
    "le = LabelEncoder()\n",
    "df.loc[:,\"Page\"] = le.fit_transform(df[\"Page\"])\n",
    "\n",
    "random.seed(2)\n",
    "random_pages = random.sample(sorted(df[\"Page\"].unique()), 50000)\n",
    "df = df.loc[df[\"Page\"].isin(random_pages)]\n",
    "'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to speed up experiments\n",
    "#df.to_csv(\"data/my_df.csv\", index=False, encoding = 'UTF-8')\n",
    "#df = pd.read_csv(\"data/my_df.csv\", parse_dates=[\"date\"], dtype={\"Page\":'int64', \"Visits\": 'float32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import math\n",
    "\n",
    "@jit\n",
    "def smape_fast(y_true, y_pred):\n",
    "    out = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        a = y_true[i]\n",
    "        b = y_pred[i]\n",
    "        if b < 1:\n",
    "            b = 0\n",
    "        c = a+b\n",
    "        if c == 0:\n",
    "            continue\n",
    "        out += math.fabs(a - b) / c\n",
    "    out *= (200.0 / y_true.shape[0])\n",
    "    return out\n",
    "\n",
    "def lgb_smape(preds, df):\n",
    "    labels = df.get_label()\n",
    "    labels, preds = np.expm1(np.array(labels)), np.expm1(np.array(preds))\n",
    "    return 'smape', smape_fast(labels, preds), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  def create_time_features(data):\n",
    "    print(\"Creating counters....\")\n",
    "    data['weekday'] = data.date.dt.weekday\n",
    "    data['is_weekend'] = ((data.date.dt.dayofweek) // 5 == 1).astype('int32')\n",
    "    data['year'] = data.date.dt.year \n",
    "    data['month'] = data.date.dt.month\n",
    "    data['day'] = data.date.dt.day\n",
    "    data['dayCount'] = data['date'].apply(lambda x: x.toordinal())\n",
    "    data['weekOfYear'] = data.date.dt.weekofyear\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating counters....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#little engeneering\n",
    "df.Visits.fillna(0, inplace=True)\n",
    "df = create_time_features(df)\n",
    "df[\"Visits_log\"] = np.log1p(df[\"Visits\"])\n",
    "df['yearminusone'] = df['year'] - 1\n",
    "\n",
    "#add to every datapoint visits from one year ago\n",
    "df = pd.merge(df, df[['Page','year','month','day','Visits_log']], left_on =['Page','yearminusone','month','day'],\\\n",
    "                     right_on = ['Page','year','month','day'], how = 'left', suffixes=('', '_2015'), sort = False)\n",
    "\n",
    "df.drop(['yearminusone','year_2015','year'], axis = 1, inplace = True)\n",
    "\n",
    "df = df.loc[df.date >= '2015-11-11']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.to_pickle(\"data/my_df.pcl\")\n",
    "#df = pd.read_pickle(\"data/my_df.pcl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepareData(data, train_start_date, train_end_date, test_start_date, test_end_date):\n",
    "    data.sort_values(by = \"dayCount\", inplace = True)\n",
    "    data = data.reset_index(drop = True)\n",
    "    \n",
    "    train_start_index = data.loc[data.date == train_start_date].index[0]\n",
    "    train_end_index = data.loc[data.date == train_end_date].index[-1]\n",
    "    test_start_index = data.loc[data.date == test_start_date].index[0]\n",
    "    test_end_index = data.loc[data.date == test_end_date].index[-1]\n",
    "   \n",
    "    last60days_index = data.loc[data.date == (pd.to_datetime(train_end_date) - relativedelta(days = 60))].index[0]\n",
    "    last30days_index = data.loc[data.date == (pd.to_datetime(train_end_date) - relativedelta(days = 30))].index[0]\n",
    "    \n",
    "    \n",
    "    #dirty hack: делаем в тестовом сете из остатка 11-го месяца - 10-й \n",
    "    data.loc[((data.month == 11) & (data.index >=test_start_index) & (data.index <= test_end_index)),\"month\"] = 10 \n",
    "    \n",
    "    # just for simplify\n",
    "    data.rename(columns={\"Visits\":\"y\", \"Visits_log\":\"y_log\"}, inplace=True)\n",
    "    \n",
    "    print(\"Calculate averages....\")\n",
    "    # считаем средние только по тренировочной части, чтобы избежать лика\n",
    "    # \n",
    "    temp = pd.DataFrame(data.loc[train_start_index:train_end_index].groupby(['Page','weekday'])['y_log'].median())\n",
    "    temp.columns = ['weekday_average']\n",
    "    data = data.join(temp, on =['Page','weekday'], how = 'left', sort = False)\n",
    "             \n",
    "    temp = pd.DataFrame(data.loc[train_start_index:train_end_index].groupby(['Page'])['y_log'].median())\n",
    "    temp.columns = ['Page_average']\n",
    "    data = data.join(temp, on =['Page'], how = 'left', sort = False)\n",
    "           \n",
    "    temp = pd.DataFrame(data.loc[train_start_index:train_end_index].groupby(['Page'])['y_log'].quantile(0.95))\n",
    "    temp.columns = ['quant_95']\n",
    "    data = data.join(temp, on =['Page'], how = 'left', sort = False)  \n",
    "    \n",
    "    temp = pd.DataFrame(data.groupby(['Page','weekOfYear'])['y_log'].median())\n",
    "    temp.columns = ['week_0']\n",
    "    temp['week_10'] = temp.week_0.shift(10)\n",
    "    temp['week_11'] = temp.week_0.shift(11)\n",
    "    temp['week_12'] = temp.week_0.shift(12)    \n",
    "    data = data.join(temp, on =['Page','weekOfYear'], how = 'left', sort = False)\n",
    "      \n",
    "    \n",
    "    #temp = pd.DataFrame(data.loc[last60days_index:train_end_index].groupby(['Page'])['y_log'].std())\n",
    "    #temp.columns = ['std_dev']\n",
    "    #data = data.join(temp, on =['Page'], how = 'left', sort = False)  \n",
    "    \n",
    "    \n",
    "      \n",
    "    #make feature \"month number to predict\": first of second\n",
    "    data['test_month_no'] = ((data.month % 2 == 0) + 1).astype('int32')\n",
    "    \n",
    "    # фича - для первого месяца предикта подцепляем медиану минус один месяц назад, для второго - минус 2 месяца назад\n",
    "    temp = pd.DataFrame(data.groupby(['Page','month','test_month_no'])['y_log'].median())\n",
    "    temp.columns = ['month_0']\n",
    "    temp['month_1'] = temp.month_0.shift(1)\n",
    "    temp['month_2'] = temp.month_0.shift(2)\n",
    "    temp['month_3'] = temp.month_0.shift(3)\n",
    "    \n",
    "    temp = temp.reset_index()\n",
    "    temp['month_1'] = temp.month_0.shift(1)\n",
    "    temp['month_2'] = temp.month_0.shift(2)\n",
    "    temp[\"last_month\"] = 0   \n",
    "    temp.loc[temp.test_month_no == 1,\"last_month\"] = temp.month_1\n",
    "    temp.loc[temp.test_month_no == 2,\"last_month\"] = temp.month_2\n",
    "    temp.set_index(['Page', 'month', 'test_month_no'], inplace=True)    \n",
    "    data = data.join(temp, on =['Page','month','test_month_no'], how = 'left', sort = False)\n",
    "    \n",
    "    data.sort_values(by = \"dayCount\", inplace = True)\n",
    "    data = data.reset_index(drop = True)\n",
    "    \n",
    "    features = ['Page','Page_average','date','is_weekend', 'dayCount','weekday_average',\\\n",
    "                'week_10','last_month','month','Visits_log_2015', 'quant_95','test_month_no', 'y_log']\n",
    "    return data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_test_split(data, train_start_date,train_end_date, test_start_date, test_end_date):\n",
    "    data.sort_values(by = \"dayCount\", inplace = True)\n",
    "    data = data.reset_index(drop = True)\n",
    "    train_start_index = data.loc[data.date == train_start_date].index[0]\n",
    "    train_end_index = data.loc[data.date == train_end_date].index[-1]\n",
    "    test_start_index = data.loc[data.date == test_start_date].index[0]\n",
    "    test_end_index = data.loc[data.date == test_end_date].index[-1]\n",
    "\n",
    "    print(\"Splitting to train - test....\")\n",
    "    X_train = data.loc[train_start_index:train_end_index,:].copy()\n",
    "    \n",
    "    #remove outliers\n",
    "    X_train.loc[(X_train.y_log > X_train.quant_95),\"y_log\"] = X_train.quant_95\n",
    "       \n",
    "    y_train = X_train.loc[:,\"y_log\"]\n",
    "    X_train = X_train.drop([\"y_log\"], axis=1)\n",
    "       \n",
    "    X_test = data.loc[test_start_index:test_end_index,:].copy()\n",
    "    y_test = X_test.loc[:,\"y_log\"]\n",
    "    X_test = X_test.drop([\"y_log\"], axis=1)\n",
    "   \n",
    "    print(\"Splitting done\")\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate averages....\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train_start_date = '2016-01-01'\n",
    "train_start_date = '2016-01-01'\n",
    "train_end_date  = '2016-08-31'\n",
    "test_start_date  = '2016-09-10'\n",
    "test_end_date  = '2016-11-10'\n",
    "dates = [train_start_date, train_end_date,test_start_date, test_end_date]\n",
    "dataForModel = prepareData(df.copy(), *dates)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataForModel.Visits_log_2015.fillna(dataForModel.last_month, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting to train - test....\n",
      "Splitting done\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train,X_test,y_test = train_test_split(dataForModel, *dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's smape: 121.984\tvalid_1's smape: 121.276\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\ttraining's smape: 115.856\tvalid_1's smape: 115.299\n",
      "[3]\ttraining's smape: 109.747\tvalid_1's smape: 109.381\n",
      "[4]\ttraining's smape: 109.746\tvalid_1's smape: 109.413\n",
      "[5]\ttraining's smape: 105.18\tvalid_1's smape: 103.849\n",
      "[6]\ttraining's smape: 100.762\tvalid_1's smape: 98.4604\n",
      "[7]\ttraining's smape: 96.533\tvalid_1's smape: 93.3888\n",
      "[8]\ttraining's smape: 90.9696\tvalid_1's smape: 88.269\n",
      "[9]\ttraining's smape: 85.6938\tvalid_1's smape: 83.2946\n",
      "[10]\ttraining's smape: 82.8651\tvalid_1's smape: 79.9919\n",
      "[11]\ttraining's smape: 79.6259\tvalid_1's smape: 76.1508\n",
      "[12]\ttraining's smape: 79.6227\tvalid_1's smape: 76.1695\n",
      "[13]\ttraining's smape: 79.2981\tvalid_1's smape: 75.8526\n",
      "[14]\ttraining's smape: 74.8982\tvalid_1's smape: 72.1568\n",
      "[15]\ttraining's smape: 70.8534\tvalid_1's smape: 68.836\n",
      "[16]\ttraining's smape: 67.1454\tvalid_1's smape: 65.8523\n",
      "[17]\ttraining's smape: 63.8359\tvalid_1's smape: 63.2573\n",
      "[18]\ttraining's smape: 62.0047\tvalid_1's smape: 61.1286\n",
      "[19]\ttraining's smape: 59.2992\tvalid_1's smape: 59.2108\n",
      "[20]\ttraining's smape: 56.9102\tvalid_1's smape: 57.3847\n",
      "[21]\ttraining's smape: 55.7476\tvalid_1's smape: 56.1038\n",
      "[22]\ttraining's smape: 53.8464\tvalid_1's smape: 54.9725\n",
      "[23]\ttraining's smape: 52.9997\tvalid_1's smape: 53.9353\n",
      "[24]\ttraining's smape: 52.3071\tvalid_1's smape: 53.1895\n",
      "[25]\ttraining's smape: 50.9511\tvalid_1's smape: 52.358\n",
      "[26]\ttraining's smape: 49.7202\tvalid_1's smape: 51.7972\n",
      "[27]\ttraining's smape: 48.2974\tvalid_1's smape: 50.8893\n",
      "[28]\ttraining's smape: 47.5965\tvalid_1's smape: 49.9213\n",
      "[29]\ttraining's smape: 47.2905\tvalid_1's smape: 49.6481\n",
      "[30]\ttraining's smape: 47.0396\tvalid_1's smape: 49.3878\n",
      "[31]\ttraining's smape: 46.6854\tvalid_1's smape: 49.0618\n",
      "[32]\ttraining's smape: 45.7057\tvalid_1's smape: 48.5957\n",
      "[33]\ttraining's smape: 45.4995\tvalid_1's smape: 48.3102\n",
      "[34]\ttraining's smape: 45.3748\tvalid_1's smape: 48.1673\n",
      "[35]\ttraining's smape: 45.2684\tvalid_1's smape: 47.752\n",
      "[36]\ttraining's smape: 45.0711\tvalid_1's smape: 47.5958\n",
      "[37]\ttraining's smape: 45.0023\tvalid_1's smape: 47.5198\n",
      "[38]\ttraining's smape: 45.0006\tvalid_1's smape: 47.5089\n",
      "[39]\ttraining's smape: 44.9905\tvalid_1's smape: 47.5009\n",
      "[40]\ttraining's smape: 44.9246\tvalid_1's smape: 47.4496\n",
      "[41]\ttraining's smape: 44.8932\tvalid_1's smape: 47.4726\n",
      "[42]\ttraining's smape: 44.8205\tvalid_1's smape: 47.4121\n",
      "[43]\ttraining's smape: 44.2624\tvalid_1's smape: 47.3662\n",
      "[44]\ttraining's smape: 44.2465\tvalid_1's smape: 47.3903\n",
      "[45]\ttraining's smape: 44.2241\tvalid_1's smape: 47.3324\n",
      "[46]\ttraining's smape: 44.2147\tvalid_1's smape: 47.3178\n",
      "[47]\ttraining's smape: 44.2077\tvalid_1's smape: 47.3127\n",
      "[48]\ttraining's smape: 44.2017\tvalid_1's smape: 47.2701\n",
      "[49]\ttraining's smape: 44.1958\tvalid_1's smape: 47.2638\n",
      "[50]\ttraining's smape: 44.1845\tvalid_1's smape: 47.2462\n",
      "[51]\ttraining's smape: 44.1786\tvalid_1's smape: 47.1958\n",
      "[52]\ttraining's smape: 43.7226\tvalid_1's smape: 47.1656\n",
      "[53]\ttraining's smape: 43.7328\tvalid_1's smape: 47.1735\n",
      "[54]\ttraining's smape: 43.7325\tvalid_1's smape: 47.1692\n",
      "[55]\ttraining's smape: 43.3606\tvalid_1's smape: 47.1659\n",
      "[56]\ttraining's smape: 43.3592\tvalid_1's smape: 47.145\n",
      "[57]\ttraining's smape: 43.3735\tvalid_1's smape: 47.1588\n",
      "[58]\ttraining's smape: 43.1158\tvalid_1's smape: 47.126\n",
      "[59]\ttraining's smape: 43.1135\tvalid_1's smape: 47.0943\n",
      "[60]\ttraining's smape: 42.9022\tvalid_1's smape: 47.0965\n",
      "[61]\ttraining's smape: 42.9031\tvalid_1's smape: 47.0929\n",
      "[62]\ttraining's smape: 42.6224\tvalid_1's smape: 47.0996\n",
      "[63]\ttraining's smape: 42.6248\tvalid_1's smape: 47.1089\n",
      "[64]\ttraining's smape: 42.4778\tvalid_1's smape: 47.1004\n",
      "[65]\ttraining's smape: 42.2441\tvalid_1's smape: 47.1053\n",
      "[66]\ttraining's smape: 42.1244\tvalid_1's smape: 47.1003\n",
      "[67]\ttraining's smape: 42.1126\tvalid_1's smape: 47.1261\n",
      "[68]\ttraining's smape: 41.9405\tvalid_1's smape: 47.1306\n",
      "[69]\ttraining's smape: 41.9398\tvalid_1's smape: 47.133\n",
      "[70]\ttraining's smape: 41.9347\tvalid_1's smape: 47.1209\n",
      "[71]\ttraining's smape: 41.9335\tvalid_1's smape: 47.1164\n",
      "[72]\ttraining's smape: 41.9332\tvalid_1's smape: 47.1135\n",
      "[73]\ttraining's smape: 41.7985\tvalid_1's smape: 47.1166\n",
      "[74]\ttraining's smape: 41.6781\tvalid_1's smape: 47.1368\n",
      "[75]\ttraining's smape: 41.6715\tvalid_1's smape: 47.1252\n",
      "[76]\ttraining's smape: 41.6676\tvalid_1's smape: 47.1206\n",
      "[77]\ttraining's smape: 41.6611\tvalid_1's smape: 47.1418\n",
      "[78]\ttraining's smape: 41.6555\tvalid_1's smape: 47.134\n",
      "[79]\ttraining's smape: 41.6545\tvalid_1's smape: 47.1337\n",
      "[80]\ttraining's smape: 41.5837\tvalid_1's smape: 47.1222\n",
      "[81]\ttraining's smape: 41.498\tvalid_1's smape: 47.1241\n",
      "[82]\ttraining's smape: 41.4481\tvalid_1's smape: 47.1431\n",
      "[83]\ttraining's smape: 41.4353\tvalid_1's smape: 47.1548\n",
      "[84]\ttraining's smape: 41.3707\tvalid_1's smape: 47.1607\n",
      "[85]\ttraining's smape: 41.316\tvalid_1's smape: 47.1783\n",
      "[86]\ttraining's smape: 41.2422\tvalid_1's smape: 47.1705\n",
      "[87]\ttraining's smape: 41.1733\tvalid_1's smape: 47.1505\n",
      "[88]\ttraining's smape: 41.1715\tvalid_1's smape: 47.1504\n",
      "[89]\ttraining's smape: 41.1654\tvalid_1's smape: 47.1483\n",
      "[90]\ttraining's smape: 41.164\tvalid_1's smape: 47.1488\n",
      "[91]\ttraining's smape: 41.1257\tvalid_1's smape: 47.1526\n",
      "[92]\ttraining's smape: 41.0911\tvalid_1's smape: 47.1564\n",
      "[93]\ttraining's smape: 41.058\tvalid_1's smape: 47.1713\n",
      "[94]\ttraining's smape: 41.0344\tvalid_1's smape: 47.1747\n",
      "[95]\ttraining's smape: 41.0136\tvalid_1's smape: 47.1712\n",
      "[96]\ttraining's smape: 41.0034\tvalid_1's smape: 47.172\n",
      "[97]\ttraining's smape: 40.993\tvalid_1's smape: 47.1893\n",
      "[98]\ttraining's smape: 40.983\tvalid_1's smape: 47.1898\n",
      "[99]\ttraining's smape: 40.9684\tvalid_1's smape: 47.2066\n",
      "[100]\ttraining's smape: 40.9412\tvalid_1's smape: 47.2036\n",
      "[101]\ttraining's smape: 40.9311\tvalid_1's smape: 47.1763\n",
      "[102]\ttraining's smape: 40.9241\tvalid_1's smape: 47.1882\n",
      "[103]\ttraining's smape: 40.9035\tvalid_1's smape: 47.1916\n",
      "[104]\ttraining's smape: 40.9041\tvalid_1's smape: 47.1919\n",
      "[105]\ttraining's smape: 40.8977\tvalid_1's smape: 47.19\n",
      "[106]\ttraining's smape: 40.8902\tvalid_1's smape: 47.1736\n",
      "[107]\ttraining's smape: 40.8902\tvalid_1's smape: 47.1706\n",
      "[108]\ttraining's smape: 40.884\tvalid_1's smape: 47.1799\n",
      "[109]\ttraining's smape: 40.879\tvalid_1's smape: 47.1781\n",
      "[110]\ttraining's smape: 40.8667\tvalid_1's smape: 47.1757\n",
      "[111]\ttraining's smape: 40.8628\tvalid_1's smape: 47.1736\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's smape: 42.9031\tvalid_1's smape: 47.0929\n"
     ]
    }
   ],
   "source": [
    "# features used to train model \n",
    "features = ['Page','is_weekend', 'dayCount','weekday_average',\\\n",
    "                'last_month','test_month_no','month','Visits_log_2015']\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train[features], label=y_train, free_raw_data=False)\n",
    "lgb_val = lgb.Dataset(X_test[features], label=y_test, free_raw_data=False, reference=lgb_train)\n",
    "\n",
    "model = lgb.train(param, lgb_train, 300, valid_sets=[lgb_train,lgb_val], feval=lgb_smape, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
